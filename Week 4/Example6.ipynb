{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6f7491-a18d-4304-bb3a-d99fbf472715",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hv/2kc3n6ks0dg9vtz1r5nrsqq40000gn/T/ipykernel_63290/2927965386.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Make Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_notebook_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "\n",
    "## Make Imports \n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from IPython.core.display import HTML\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import IPython.display\n",
    "from PIL import Image \n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import warnings\n",
    "import operator\n",
    "import string \n",
    "import re \n",
    "\n",
    "## Environment Preparation\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "init_notebook_mode(connected=True)\n",
    "stopwords = stopwords.words(\"english\")\n",
    "punc = string.punctuation \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "## Load dataset\n",
    "user_followers = pd.read_csv(\"../input/UserFollowers.csv\")\n",
    "kernels_tag_df = pd.read_csv(\"../input/KernelTags.csv\")\n",
    "messages = pd.read_csv(\"../input/ForumMessages.csv\")\n",
    "forums_df = pd.read_csv(\"../input/Forums.csv\")\n",
    "kernels_df = pd.read_csv(\"../input/Kernels.csv\")\n",
    "tags_df = pd.read_csv(\"../input/Tags.csv\")\n",
    "users = pd.read_csv(\"../input/Users.csv\")\n",
    "\n",
    "## function to clean the text\n",
    "def clean_text(txt):    \n",
    "    txt = txt.lower()\n",
    "    txt = re.sub('<[^<]+?>', '', txt)\n",
    "    txt = \"\".join(x for x in txt if x not in punc)\n",
    "    words = txt.split()\n",
    "    words = [wrd for wrd in words if wrd not in stopwords]\n",
    "    words = [wrd for wrd in words if not wrd.startswith(\"http\")]\n",
    "    txt = \" \".join(words)\n",
    "    return txt\n",
    "\n",
    "## function to generate ngrams\n",
    "def ngrams(txt, n):\n",
    "    txt = txt.split()\n",
    "    output = []\n",
    "    for i in range(len(txt)-n+1):\n",
    "        output.append(\" \".join(txt[i:i+n]))\n",
    "    return output\n",
    "\n",
    "## function to create bins \n",
    "def wordmap(val):\n",
    "    if val < 10:\n",
    "        return \"0-10\"\n",
    "    elif val < 20:\n",
    "        return \"10-25\"\n",
    "    elif val < 50:\n",
    "        return \"25-50\"\n",
    "    elif val < 100:\n",
    "        return \"50-100\"\n",
    "    else:\n",
    "        return \"100+\"\n",
    "\n",
    "## main Analysis Function\n",
    "def _analyze_profile(username):\n",
    "    account_id = users[users[\"UserName\"] == username][\"Id\"].iloc(0)[0]\n",
    "    name = users[users[\"UserName\"] == username][\"DisplayName\"].iloc(0)[0]\n",
    "\n",
    "    mydf = messages[messages[\"PostUserId\"] == account_id]\n",
    "    mydf['PostDate'] = pd.to_datetime(mydf[\"PostDate\"])\n",
    "    mydf[\"weekday\"] = mydf[\"PostDate\"].dt.weekday\n",
    "    mydf[\"monthday\"] = mydf[\"PostDate\"].dt.day\n",
    "    mydf[\"hour\"] = mydf[\"PostDate\"].dt.hour\n",
    "    mydf[\"clean_message\"] = mydf[\"Message\"].fillna(\"\").apply(lambda x : clean_text(x))\n",
    "    mydf[\"word_len\"] = mydf[\"clean_message\"].apply(lambda x : len(x.split()))\n",
    "    mydf[\"char_len\"] = mydf[\"clean_message\"].apply(lambda x : len(x.replace(\" \",\"\")))\n",
    "\n",
    "    text = \" \".join(mydf[\"clean_message\"].dropna())\n",
    "    unigrams = Counter(ngrams(text, 1))\n",
    "    bigrams = Counter(ngrams(text, 2))\n",
    "    unigrams_d = dict(unigrams)\n",
    "    bigrams_d = dict(bigrams)\n",
    "\n",
    "    mydf[\"word_len_bin\"] = mydf[\"word_len\"].apply(wordmap)\n",
    "    mydf[\"char_len_bin\"] = mydf[\"char_len\"].apply(wordmap)\n",
    "\n",
    "    mydf['Date'] = mydf['PostDate'].dt.date\n",
    "    mydf['month'] = mydf['PostDate'].dt.month\n",
    "    dateComments = mydf['Date'].value_counts().to_frame().reset_index().sort_values(\"index\")\n",
    "\n",
    "    mytags = []\n",
    "    forumIDs = list(mydf['ForumTopicId'].values)\n",
    "    for forum_id in forumIDs:\n",
    "        try:\n",
    "            kernel_id = kernels_df[kernels_df[\"ForumTopicId\"] == forum_id]['Id'].iloc(0)[0]\n",
    "            taglist = list(kernels_tag_df[kernels_tag_df[\"KernelId\"] == kernel_id][\"TagId\"].values)\n",
    "            mytags.extend(tags_df[tags_df[\"Id\"].isin(taglist)]['Name'].values)\n",
    "        except Exception as E:\n",
    "            pass\n",
    "    \n",
    "    metrics = {\"mytags\" : mytags, \"unigrams\" : unigrams, \"unigrams_d\" : unigrams_d,\n",
    "               \"bigrams\" : bigrams, \"bigrams_d\" : bigrams_d, \"dateComments\" : dateComments, \n",
    "               \"name\" : name, \"account_id\" : account_id}\n",
    "    return mydf, metrics\n",
    "\n",
    "## Main Visualization Function\n",
    "def _prepare(mydf, metrics):\n",
    "    mytags = metrics[\"mytags\"]\n",
    "    unigrams = metrics[\"unigrams\"]\n",
    "    bigrams = metrics[\"bigrams\"]\n",
    "    bigrams_d = metrics[\"bigrams_d\"]\n",
    "    unigrams_d = metrics[\"unigrams_d\"]\n",
    "    dateComments = metrics[\"dateComments\"] \n",
    "    account_id = metrics[\"account_id\"]\n",
    "    name = metrics[\"name\"]\n",
    "\n",
    "    mpp = {0 : \"Mon\", 1: \"Tue\", 2: \"Wed\", 3:\"Thu\", 4:\"Fri\", 5:\"Sat\", 6:\"Sun\"}\n",
    "    mydf[\"word_density\"] = mydf[\"char_len\"] / (1+mydf[\"word_len\"])\n",
    "    mydf[\"word_density\"] = mydf[\"word_density\"].apply(lambda x : round(x,2))\n",
    "\n",
    "    daymp = {}\n",
    "    lst = mydf[\"weekday\"].value_counts().to_frame().reset_index()\n",
    "    for l,day in lst.iterrows():\n",
    "        daymp[day[\"index\"]] = day[\"weekday\"]\n",
    "    sorted_x = sorted(daymp.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    \n",
    "    # insights_t = {\n",
    "    #     \"Total Discussions\" : len(mydf),\n",
    "    #     \"Average Discussions Per Day\" : int(np.mean(np.array(dateComments[\"Date\"].values))),\n",
    "    #     \"Average Discussions Per Month\" : int(np.mean(mydf[\"month\"].value_counts().values)),\n",
    "    #     \"Maximum Discussions on Single Day\" : int(np.max(np.array(dateComments[\"Date\"].values))),\n",
    "    #     \"Maximum Discussions Date\" : str(dateComments[dateComments[\"Date\"] == np.max(np.array(dateComments[\"Date\"].values))][\"index\"].iloc(0)[0]),\n",
    "    #     \"Most Discussions WeekDay\" : mpp[sorted_x[0][0]],\n",
    "    #     \"Least Discussions WeekDay\" : mpp[sorted_x[-1][0]],\n",
    "    #     \"Average Words Per Discussions\" : int(np.mean(mydf[\"word_len\"])),\n",
    "    #     \"Average Characters Per Discussions\" : int(np.mean(mydf[\"char_len\"])),\n",
    "    #     \"Average Word Density Per Discussions\" : round(np.mean(mydf[\"word_density\"]),2),\n",
    "    #     \"Top KeyWord Used\" : unigrams.most_common()[0][0],\n",
    "    #     \"Top Tag Followed\" : Counter(mytags).most_common(1)[0][0],\n",
    "    #     \"Total Discussions\" : len(mydf),\n",
    "    #    \"Average Discussions Per Day\" : int(np.mean(np.array(dateComments[\"Date\"].values))),\n",
    "    #    \"Average Discussions Per Month\" : int(np.mean(mydf[\"month\"].value_counts().values)),\n",
    "    #     \"Maximum Discussions on Single Day\" : int(np.max(np.array(dateComments[\"Date\"].values))),\n",
    "    #     \"Maximum Discussions Date\" : str(dateComments[dateComments[\"Date\"] == np.max(np.array(dateComments[\"Date\"].values))][\"index\"].iloc(0)[0]),\n",
    "    #    \"Most Discussions WeekDay\" : mpp[sorted_x[0][0]],\n",
    "    #    \"Least Discussions WeekDay\" : mpp[sorted_x[-1][0]],\n",
    "    #    \"Average Words Per Discussions\" : int(np.mean(mydf[\"word_len\"])),\n",
    "    #    \"Average Characters Per Discussions\" : int(np.mean(mydf[\"char_len\"])),\n",
    "    #    \"Average Word Density Per Discussions\" : round(np.mean(mydf[\"word_density\"]),2),\n",
    "    #    \"Top KeyWord Used\" : unigrams.most_common()[0][0],\n",
    "    #    \"Top Tag Followed\" : Counter(mytags).most_common(1)[0][0],\n",
    "    #}\n",
    "    \n",
    "    insights = {\n",
    "        \"Total Discussions\" : len(mydf),\n",
    "        \"Average Discussions Per Day\" : int(np.mean(np.array(dateComments[\"Date\"].values))),\n",
    "        \"Average Discussions Per Month\" : int(np.mean(mydf[\"month\"].value_counts().values)),\n",
    "        \"Maximum Discussions on Single Day\" : int(np.max(np.array(dateComments[\"Date\"].values))),\n",
    "        \"Maximum Discussions Date\" : str(dateComments[dateComments[\"Date\"] == np.max(np.array(dateComments[\"Date\"].values))][\"index\"].iloc(0)[0]),\n",
    "        \"Most Discussions WeekDay\" : mpp[sorted_x[0][0]],\n",
    "        \"Least Discussions WeekDay\" : mpp[sorted_x[-1][0]],\n",
    "        \"Average Words Per Discussions\" : int(np.mean(mydf[\"word_len\"])),\n",
    "        \"Average Characters Per Discussions\" : int(np.mean(mydf[\"char_len\"])),\n",
    "        \"Average Word Density Per Discussions\" : round(np.mean(mydf[\"word_density\"]),2),\n",
    "        \"Top KeyWord Used\" : unigrams.most_common()[0][0],\n",
    "        \"Top Tag Followed\" : Counter(mytags).most_common(1)[0][0],\n",
    "    }\n",
    "\n",
    "    tabs1 = list(insights.keys())\n",
    "    tabvals1 = list(insights.values())\n",
    "    tr9 = go.Table(header=dict(values=['Metric', 'Value'], line = dict(color='#7D7F80'), fill = dict(color='#a1c3d1'), align = ['left'] * 2),\n",
    "                     cells=dict(values=[tabs1, tabvals1], line = dict(color='#7D7F80'), fill = dict(color='#EDFAFF'), align = ['left'] * 2))\n",
    "\n",
    "    layout = dict(title=\"Report Summary : \" + name, height=500)\n",
    "    data = [tr9]\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    iplot(fig)\n",
    "\n",
    "    lst = unigrams.most_common(15)\n",
    "    tr1 = go.Bar(y= [c[0] for c in lst][::-1], x= [c[1] for c in lst][::-1], orientation=\"h\") \n",
    "    lst = bigrams.most_common(15)\n",
    "    tr2 = go.Bar(y= [c[0] for c in lst][::-1], x= [c[1] for c in lst][::-1], orientation=\"h\") \n",
    "\n",
    "    dvals = [daymp[0], daymp[1], daymp[2], daymp[3], daymp[4], daymp[5], daymp[6]]\n",
    "    tr3 = go.Bar(x= [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"], y= dvals, marker=dict(color=\"#c7a6ea\")) \n",
    "    lst = mydf[\"monthday\"].value_counts()\n",
    "    tr4 = go.Bar(x= lst.index, y= lst.values, marker=dict(color=\"#c7a6ea\")) \n",
    "\n",
    "    lst = dict(mydf[\"word_len_bin\"].value_counts())\n",
    "    xx = [\"0-10\", \"10-25\", \"25-50\", \"50-100\", \"100+\"]\n",
    "    yy = [lst[v] for v in xx]\n",
    "    tr5 = go.Bar(x= xx, y= yy, marker=dict(color=\"#eaa5b8\")) \n",
    "    lst = mydf[\"char_len_bin\"].value_counts()\n",
    "    yy = [lst[v] for v in xx]\n",
    "    tr6 = go.Bar(x= xx, y= yy, marker=dict(color=\"#eaa5b8\")) \n",
    "\n",
    "    lst = Counter(mytags).most_common(15)\n",
    "    tr7 = go.Scatter(x=[c+3 for c in range(len(lst))], y = [c[1] for c in lst], text=[c[0] for c in lst], \n",
    "                     textposition='top right', textfont=dict(size=10), mode='markers+text', marker=dict(color=\"#42f4a4\", size=25 ))\n",
    "\n",
    "    fig = tools.make_subplots(rows=5, cols=2, vertical_spacing = 0.05, print_grid=False, specs = [[{\"colspan\" : 2},None], [{},{}], [{},{}], [{\"colspan\" : 2},None], [{\"colspan\" : 2},None]], \n",
    "                             subplot_titles=[\"Discussions by Date\", \"Day of the Week - Discussion Activity\", \"Day of the Month - Discussion Activity\",\n",
    "                                             \"Number of Words per Discussions\", \"Number of Characters used per Discussions\", \n",
    "                                             \"Top Kernel Tags Followed\", \"Followers Gained Since 2017\" ])\n",
    "\n",
    "    tr8 = go.Scatter(x = dateComments[\"index\"], y = dateComments[\"Date\"], mode=\"lines+markers\", line=dict(color=\"orange\", width=3))\n",
    "    myfol = user_followers[user_followers[\"FollowingUserId\"] == account_id]\n",
    "    myfol[\"Date\"] = pd.to_datetime(myfol[\"CreationDate\"])\n",
    "    tmp = myfol[\"Date\"].value_counts().to_frame().reset_index().sort_values(\"index\")\n",
    "    tr10 = go.Scatter(x = tmp[\"index\"], y = tmp[\"Date\"], mode=\"lines+markers\", line=dict(color=\"pink\", width=3))\n",
    "\n",
    "    fig.append_trace(tr8, 1, 1)\n",
    "    fig.append_trace(tr3, 2, 1)\n",
    "    fig.append_trace(tr4, 2, 2)\n",
    "    fig.append_trace(tr5, 3, 1)\n",
    "    fig.append_trace(tr6, 3, 2)\n",
    "    fig.append_trace(tr7, 4, 1)\n",
    "    fig.append_trace(tr10, 5, 1)\n",
    "\n",
    "    fig['layout'].update(barmode='group', title = 'Kaggle Discussions Analysis Report: ' + name,\n",
    "        titlefont=dict(size=22,color='#000'),                     \n",
    "        margin=dict(t=100, b=100),\n",
    "        paper_bgcolor='rgb(254, 247, 234)',\n",
    "        plot_bgcolor='rgb(254, 247, 234)',\n",
    "        height=1300,\n",
    "        showlegend=False)\n",
    "    iplot(fig);\n",
    "    \n",
    "## main wordcloud function\n",
    "def _wc(mydf):\n",
    "    wordcloud = WordCloud(max_font_size=40, max_words=12000, colormap='Dark2_r', random_state=42).generate(str(mydf['clean_message']))\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(\"Top Used Words\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "## Function to generate Ngram Bubble Cloud\n",
    "def _ngramCloud(bigrams, username):\n",
    "    strr = \"id,value,value1\\nproject,\\n\"\n",
    "    num = 1\n",
    "    cnt = 1\n",
    "    sizes =[9000,7500,6000,5000,4000,2500,2200,1900,1800,1860]\n",
    "    for j, each in enumerate(bigrams.most_common(100)):\n",
    "        val = each[1]\n",
    "        strr += \"project.\" +str(num)+\".\"+ str(each[0]) + \",\" + str(val) + \",\" + str(val) + \"\\n\"\n",
    "        if cnt % 2 == 0:\n",
    "            num += 1\n",
    "        cnt += 1\n",
    "        if cnt == 100:\n",
    "            break\n",
    "    fout = open(\"flare\"+username+\".csv\", \"w\")\n",
    "    fout.write(strr)\n",
    "\n",
    "    html_p1 = \"\"\"<!DOCTYPE html><svg id='idd_\"\"\"+username+\"\"\"' width=\"760\" height=\"760\" font-family=\"sans-serif\" font-size=\"10\" text-anchor=\"middle\"></svg>\"\"\"\n",
    "    js_p1 = \"\"\"require.config({paths: {d3: \"https://d3js.org/d3.v4.min\"}});\n",
    "    require([\"d3\"], function(d3) {var svg=d3.select(\"#idd_\"\"\"+username+\"\"\"\"),width=+svg.attr(\"width\"),height=+svg.attr(\"height\"),format=d3.format(\",d\"),color=d3.scaleOrdinal(d3.schemeCategory20c);var pack=d3.pack().size([width,height]).padding(1.5);d3.csv(\"flare\"\"\"+username+\"\"\".csv\",function(t){if(t.value=+t.value,t.value)return t},function(t,e){if(t)throw t;var n=d3.hierarchy({children:e}).sum(function(t){return t.value}).each(function(t){if(e=t.data.id){var e,n=e.lastIndexOf(\".\");t.id=e,t.package=e.slice(0,n),t.class=e.slice(n+1)}}),a=(d3.select(\"body\").append(\"div\").style(\"position\",\"absolute\").style(\"z-index\",\"10\").style(\"visibility\",\"hidden\").text(\"a\"),svg.selectAll(\".node\").data(pack(n).leaves()).enter().append(\"g\").attr(\"class\",\"node\").attr(\"transform\",function(t){return\"translate(\"+t.x+\",\"+t.y+\")\"}));a.append(\"circle\").attr(\"id\",function(t){return t.id}).attr(\"r\",function(t){return t.r}).style(\"fill\",function(t){return color(t.package)}),a.append(\"clipPath\").attr(\"id\",function(t){return\"clip-\"+t.id}).append(\"use\").attr(\"xlink:href\",function(t){return\"#\"+t.id}),a.append(\"svg:title\").text(function(t){return t.value}),a.append(\"text\").attr(\"clip-path\",function(t){return\"url(#clip-\"+t.id+\")\"}).selectAll(\"tspan\").data(function(t){return t.class.split(/(?=[A-Z][^A-Z])/g)}).enter().append(\"tspan\").attr(\"x\",0).attr(\"y\",function(t,e,n){return 13+10*(e-n.length/2-.5)}).text(function(t){return t})});});\"\"\"\n",
    "    h = display(HTML(html_p1))\n",
    "    j = IPython.display.Javascript(js_p1)\n",
    "    IPython.display.display_javascript(j)\n",
    "\n",
    "## master function to generate the complete report \n",
    "def _generate_report(username):\n",
    "    mydf, metrics = _analyze_profile(username)\n",
    "    _prepare(mydf, metrics)\n",
    "    display(HTML(\"Top Ngrams Used\"))\n",
    "    _ngramCloud(metrics[\"bigrams\"], username)\n",
    "    _wc(mydf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67404ab2-a6f9-490f-86a1-96d1efc8bac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
